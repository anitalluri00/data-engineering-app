import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import time
import os
from dotenv import load_dotenv

# Import custom modules
from database.models import DataStorage
from ingestion.file_processor import FileProcessor
from ingestion.web_crawler import WebCrawler
from processing.etl_pipeline import ETLPipeline
from analytics.reporting import AnalyticsReporter
from analytics.ml_models import MLModels
from utils.helpers import Helpers

# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(
    page_title="Data Engineering Platform",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize classes
@st.cache_resource
def init_database():
    return DataStorage()

@st.cache_resource
def init_file_processor():
    return FileProcessor()

@st.cache_resource
def init_web_crawler():
    return WebCrawler(delay=1.0)

@st.cache_resource
def init_etl_pipeline():
    return ETLPipeline()

@st.cache_resource
def init_analytics_reporter():
    return AnalyticsReporter()

@st.cache_resource
def init_ml_models():
    return MLModels()

# Authentication
def check_password():
    """Returns `True` if the user had the correct password."""
    def password_entered():
        """Checks whether a password entered by the user is correct."""
        if st.session_state["password"] == os.getenv("ADMIN_PASSWORD", "admin123"):
            st.session_state["password_correct"] = True
            del st.session_state["password"]  # Don't store password
        else:
            st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        # First run, show input for password.
        st.text_input(
            "Password", type="password", on_change=password_entered, key="password"
        )
        return False
    elif not st.session_state["password_correct"]:
        # Password not correct, show input + error.
        st.text_input(
            "Password", type="password", on_change=password_entered, key="password"
        )
        st.error("ðŸ˜• Password incorrect")
        return False
    else:
        # Password correct.
        return True

def main():
    if not check_password():
        st.stop()
    
    # Initialize components
    db = init_database()
    file_processor = init_file_processor()
    web_crawler = init_web_crawler()
    etl_pipeline = init_etl_pipeline()
    analytics_reporter = init_analytics_reporter()
    ml_models = init_ml_models()
    
    # Initialize database tables
    try:
        db.initialize_database()
    except Exception as e:
        st.error(f"Database initialization error: {e}")
    
    # Sidebar
    st.sidebar.title("Data Engineering Platform")
    st.sidebar.markdown("---")
    
    menu = st.sidebar.selectbox(
        "Navigation",
        ["Dashboard", "Data Ingestion", "Web Crawling", "ETL Pipeline", 
         "Machine Learning", "Data Quality", "Analytics", "Database Explorer"]
    )
    
    # System Info in Sidebar
    st.sidebar.markdown("---")
    st.sidebar.subheader("System Information")
    
    try:
        from database.connection import db_connection
        conn = db_connection.get_connection()
        cursor = conn.cursor(dictionary=True)
        
        cursor.execute("SELECT COUNT(*) as total_files FROM files")
        total_files = cursor.fetchone()['total_files']
        
        cursor.execute("SELECT COUNT(*) as processed_files FROM files WHERE processed = TRUE")
        processed_files = cursor.fetchone()['processed_files']
        
        cursor.close()
        
        st.sidebar.metric("Total Files", total_files)
        st.sidebar.metric("Processed Files", processed_files)
        
    except Exception as e:
        st.sidebar.error("Database connection issue")
    
    st.sidebar.markdown("---")
    st.sidebar.info(f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Navigation
    if menu == "Dashboard":
        show_dashboard(analytics_reporter)
    elif menu == "Data Ingestion":
        show_data_ingestion(file_processor)
    elif menu == "Web Crawling":
        show_web_crawling(web_crawler)
    elif menu == "ETL Pipeline":
        show_etl_pipeline(etl_pipeline)
    elif menu == "Machine Learning":
        show_machine_learning(ml_models)
    elif menu == "Data Quality":
        show_data_quality(analytics_reporter)
    elif menu == "Analytics":
        show_analytics(analytics_reporter)
    elif menu == "Database Explorer":
        show_database_explorer()

def show_dashboard(analytics_reporter):
    st.title("ðŸ“Š Data Engineering Dashboard")
    
    # Generate dashboard data
    with st.spinner("Generating dashboard data..."):
        dashboard_data = analytics_reporter.generate_dashboard_data()
    
    if not dashboard_data:
        st.error("Failed to generate dashboard data")
        return
    
    # Key metrics
    st.subheader("Key Metrics")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_files = dashboard_data['processing_stats']['total_files']
        st.metric("Total Files", total_files)
    
    with col2:
        processed_files = dashboard_data['processing_stats']['processed_files']
        st.metric("Processed Files", processed_files)
    
    with col3:
        analyzed_files = dashboard_data['processing_stats']['analyzed_files']
        st.metric("Files Analyzed", analyzed_files)
    
    with col4:
        processing_rate = dashboard_data['processing_stats']['processing_rate']
        st.metric("Processing Rate", f"{processing_rate:.1f}%")
    
    # Charts
    st.subheader("Data Overview")
    col1, col2 = st.columns(2)
    
    with col1:
        # File type distribution
        if not dashboard_data['file_type_distribution'].empty:
            fig = px.pie(
                dashboard_data['file_type_distribution'],
                values='count',
                names='file_type',
                title="File Type Distribution"
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No file type data available")
    
    with col2:
        # Data quality overview
        quality_data = dashboard_data['quality_overview']
        if quality_data:
            quality_df = pd.DataFrame([
                {'Check Type': check_type, 'Status': status, 'Count': count}
                for check_type, status_count in quality_data.items()
                for status, count in status_count.items()
            ])
            
            if not quality_df.empty:
                fig = px.bar(
                    quality_df,
                    x='Check Type',
                    y='Count',
                    color='Status',
                    title="Data Quality Overview",
                    barmode='stack'
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("No quality data available")
        else:
            st.info("No quality data available")
    
    # Recent activity
    st.subheader("Recent Activity")
    recent_activity = dashboard_data['analytics_insights']['recent_activity']
    if recent_activity:
        activity_df = pd.DataFrame(recent_activity)
        if not activity_df.empty:
            fig = px.line(
                activity_df,
                x='date',
                y='count',
                title="Processing Activity (Last 7 Days)",
                markers=True
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No recent activity data")
    else:
        st.info("No recent activity data")
    
    # Quick Actions
    st.subheader("Quick Actions")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ðŸ”„ Run ETL Pipeline", use_container_width=True):
            st.session_state.run_etl = True
            st.rerun()
    
    with col2:
        if st.button("ðŸ“¥ Process New Files", use_container_width=True):
            st.session_state.show_ingestion = True
            st.rerun()
    
    with col3:
        if st.button("ðŸ¤– Train ML Models", use_container_width=True):
            st.session_state.show_ml = True
            st.rerun()

def show_data_ingestion(file_processor):
    st.title("ðŸ“¥ Data Ingestion")
    
    st.subheader("File Upload")
    
    # File upload section
    uploaded_files = st.file_uploader(
        "Choose files to upload",
        accept_multiple_files=True,
        type=list(file_processor.supported_formats.keys()),
        help="Supported formats: " + ", ".join(
            [ext for extensions in file_processor.supported_formats.values() for ext in extensions]
        )
    )
    
    if uploaded_files:
        progress_bar = st.progress(0)
        status_text = st.empty()
        results_container = st.container()
        
        successful_uploads = 0
        failed_uploads = 0
        
        for i, uploaded_file in enumerate(uploaded_files):
            try:
                status_text.text(f"Processing {uploaded_file.name}...")
                
                # Read file data
                file_data = uploaded_file.getvalue()
                
                # Process file
                file_id, processed_data = file_processor.process_file(
                    file_data, 
                    uploaded_file.name,
                    "upload"
                )
                
                with results_container:
                    st.success(f"âœ… Processed {uploaded_file.name} (ID: {file_id})")
                
                # Show processed data preview
                with st.expander(f"View details for {uploaded_file.name}"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**Metadata:**")
                        st.json({
                            'file_id': file_id,
                            'filename': uploaded_file.name,
                            'file_size': Helpers.format_file_size(len(file_data)),
                            'file_type': processed_data.get('content_type', 'unknown')
                        })
                    
                    with col2:
                        st.write("**Processing Results:**")
                        st.json({
                            'word_count': processed_data.get('word_count', 0),
                            'char_count': processed_data.get('char_count', 0),
                            'content_preview': processed_data.get('extracted_text', '')[:200] + "..."
                        })
                
                successful_uploads += 1
                progress_bar.progress((i + 1) / len(uploaded_files))
                
            except Exception as e:
                with results_container:
                    st.error(f"âŒ Error processing {uploaded_file.name}: {str(e)}")
                failed_uploads += 1
        
        status_text.text("Processing complete!")
        
        # Summary
        st.subheader("Upload Summary")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Successful Uploads", successful_uploads)
        with col2:
            st.metric("Failed Uploads", failed_uploads)
    
    # Batch processing options
    st.subheader("Batch Processing")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Process All Unprocessed Files"):
            with st.spinner("Processing unprocessed files..."):
                try:
                    from database.connection import db_connection
                    conn = db_connection.get_connection()
                    cursor = conn.cursor(dictionary=True)
                    
                    cursor.execute("SELECT file_id, filename FROM files WHERE processed = FALSE")
                    unprocessed_files = cursor.fetchall()
                    cursor.close()
                    
                    if unprocessed_files:
                        st.info(f"Found {len(unprocessed_files)} unprocessed files")
                        # Here you would implement batch processing logic
                    else:
                        st.info("No unprocessed files found")
                        
                except Exception as e:
                    st.error(f"Error checking unprocessed files: {str(e)}")
    
    with col2:
        if st.button("Clear Processing Queue"):
            st.warning("This will reset all processing flags. Are you sure?")
            if st.button("Confirm Clear", key="confirm_clear"):
                try:
                    from database.connection import db_connection
                    conn = db_connection.get_connection()
                    cursor = conn.cursor()
                    
                    cursor.execute("UPDATE files SET processed = FALSE")
                    conn.commit()
                    cursor.close()
                    
                    st.success("Processing queue cleared")
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"Error clearing queue: {str(e)}")

def show_web_crawling(web_crawler):
    st.title("ðŸŒ Web Crawling")
    
    st.subheader("Website Crawling Configuration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        base_url = st.text_input("Base URL", "https://example.com", 
                               help="Starting URL for the crawl")
        max_pages = st.number_input("Maximum Pages", min_value=1, max_value=1000, value=50,
                                  help="Maximum number of pages to crawl")
    
    with col2:
        crawl_delay = st.number_input("Crawl Delay (seconds)", min_value=0.1, max_value=10.0, value=1.0,
                                    help="Delay between requests to be respectful to the server")
        allowed_domain = st.text_input("Allowed Domain", 
                                     value=base_url.split('//')[-1].split('/')[0] if '//' in base_url else "",
                                     help="Domain to restrict crawling to")
    
    # Advanced options
    with st.expander("Advanced Options"):
        col1, col2 = st.columns(2)
        with col1:
            respect_robots = st.checkbox("Respect robots.txt", value=True)
            follow_external_links = st.checkbox("Follow External Links", value=False)
        with col2:
            max_depth = st.number_input("Maximum Depth", min_value=1, max_value=10, value=3)
            timeout = st.number_input("Request Timeout (seconds)", min_value=5, max_value=60, value=10)
    
    if st.button("Start Crawling", type="primary"):
        if base_url and allowed_domain:
            with st.spinner(f"Crawling {base_url}..."):
                try:
                    # Configure crawler
                    web_crawler.delay = crawl_delay
                    
                    results = web_crawler.crawl_website(
                        base_url=base_url,
                        max_pages=max_pages,
                        allowed_domains=[allowed_domain]
                    )
                    
                    st.success(f"âœ… Crawling completed! Processed {len(results)} pages")
                    
                    # Show summary
                    col1, col2, col3 = st.columns(3)
                    total_words = sum(page.get('metadata', {}).get('word_count', 0) for page in results)
                    
                    with col1:
                        st.metric("Pages Crawled", len(results))
                    with col2:
                        st.metric("Total Words", total_words)
                    with col3:
                        st.metric("Unique URLs", len(set(page.get('metadata', {}).get('url') for page in results)))
                    
                    # Show results in a table
                    if results:
                        st.subheader("Crawling Results")
                        
                        results_df = pd.DataFrame([
                            {
                                'Title': page.get('metadata', {}).get('title', 'No title')[:50],
                                'URL': page.get('metadata', {}).get('url', '')[:80],
                                'Word Count': page.get('metadata', {}).get('word_count', 0),
                                'Links': page.get('metadata', {}).get('internal_links', 0)
                            }
                            for page in results
                        ])
                        
                        st.dataframe(results_df, use_container_width=True)
                        
                        # Detailed view
                        st.subheader("Page Details")
                        for i, result in enumerate(results[:5]):  # Show first 5
                            with st.expander(f"Page {i+1}: {result.get('metadata', {}).get('title', 'No title')}"):
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.write("**Metadata:**")
                                    st.json({
                                        'URL': result.get('metadata', {}).get('url'),
                                        'Title': result.get('metadata', {}).get('title'),
                                        'Crawled At': result.get('metadata', {}).get('crawled_at'),
                                        'Word Count': result.get('metadata', {}).get('word_count'),
                                        'Internal Links': result.get('metadata', {}).get('internal_links'),
                                        'External Links': result.get('metadata', {}).get('external_links')
                                    })
                                
                                with col2:
                                    st.write("**Content Preview:**")
                                    content = result.get('content', '')
                                    st.text_area("", 
                                               content[:500] + "..." if len(content) > 500 else content, 
                                               height=200,
                                               key=f"content_{i}")
                    
                except Exception as e:
                    st.error(f"âŒ Crawling failed: {str(e)}")
        else:
            st.error("Please provide both Base URL and Allowed Domain")
    
    # Multiple sources configuration
    st.subheader("Multiple Source Configuration")
    
    with st.form("multiple_sources_form"):
        st.write("Configure multiple websites to crawl")
        
        source_name = st.text_input("Source Name")
        source_url = st.text_input("Source URL")
        source_max_pages = st.number_input("Max Pages", min_value=1, max_value=500, value=100)
        
        if st.form_submit_button("Add Source"):
            if 'crawl_sources' not in st.session_state:
                st.session_state.crawl_sources = []
            
            st.session_state.crawl_sources.append({
                'name': source_name,
                'base_url': source_url,
                'max_pages': source_max_pages
            })
            st.success(f"Added source: {source_name}")
    
    # Display configured sources
    if 'crawl_sources' in st.session_state and st.session_state.crawl_sources:
        st.write("Configured Sources:")
        for i, source in enumerate(st.session_state.crawl_sources):
            col1, col2, col3 = st.columns([2, 2, 1])
            with col1:
                st.write(f"**{source['name']}**")
            with col2:
                st.write(source['base_url'])
            with col3:
                if st.button("Remove", key=f"remove_{i}"):
                    st.session_state.crawl_sources.pop(i)
                    st.rerun()
        
        if st.button("Run All Configured Crawls"):
            with st.spinner("Running configured crawls..."):
                sources_config = {
                    source['name']: {
                        'base_url': source['base_url'],
                        'max_pages': source['max_pages'],
                        'allowed_domains': [source['base_url'].split('//')[-1].split('/')[0]]
                    }
                    for source in st.session_state.crawl_sources
                }
                
                results = web_crawler.crawl_multiple_sources(sources_config)
                
                # Display results
                st.subheader("Multiple Source Results")
                for source_name, result in results.items():
                    with st.expander(f"Source: {source_name}"):
                        if result['success']:
                            st.success(f"âœ… {result['pages_crawled']} pages crawled")
                        else:
                            st.error(f"âŒ Failed: {result['error']}")

def show_etl_pipeline(etl_pipeline):
    st.title("ðŸ”„ ETL Pipeline")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Pipeline Controls")
        
        batch_size = st.number_input("Batch Size", min_value=1, max_value=1000, value=100,
                                   help="Number of files to process in each batch")
        
        st.write("Pipeline Operations:")
        
        if st.button("Run Complete ETL Pipeline", type="primary"):
            with st.spinner("Running ETL pipeline..."):
                try:
                    start_time = time.time()
                    etl_pipeline.run_pipeline(batch_size)
                    end_time = time.time()
                    
                    st.success(f"âœ… ETL pipeline completed in {end_time - start_time:.2f} seconds")
                    
                    # Show quick stats
                    from database.connection import db_connection
                    conn = db_connection.get_connection()
                    cursor = conn.cursor(dictionary=True)
                    
                    cursor.execute("SELECT COUNT(*) as new_analytics FROM analytics_results WHERE created_date > DATE_SUB(NOW(), INTERVAL 1 HOUR)")
                    new_analytics = cursor.fetchone()['new_analytics']
                    
                    cursor.execute("SELECT COUNT(*) as new_quality_checks FROM data_quality_metrics WHERE check_date > DATE_SUB(NOW(), INTERVAL 1 HOUR)")
                    new_quality = cursor.fetchone()['new_quality_checks']
                    
                    cursor.close()
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("New Analytics", new_analytics)
                    with col2:
                        st.metric("Quality Checks", new_quality)
                        
                except Exception as e:
                    st.error(f"âŒ ETL pipeline failed: {str(e)}")
        
        if st.button("Run Data Quality Checks Only"):
            with st.spinner("Running data quality checks..."):
                try:
                    # Implement quality checks only
                    st.info("Data quality checks would run here")
                    # etl_pipeline.run_quality_checks(batch_size)
                    st.success("Data quality checks completed")
                except Exception as e:
                    st.error(f"Quality checks failed: {str(e)}")
    
    with col2:
        st.subheader("Pipeline Status")
        
        # Pipeline metrics
        try:
            from database.connection import db_connection
            conn = db_connection.get_connection()
            cursor = conn.cursor(dictionary=True)
            
            # Get pipeline statistics
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_files,
                    SUM(processed) as processed_files,
                    AVG(CASE WHEN processed THEN 1 ELSE 0 END) * 100 as completion_rate
                FROM files
            """)
            pipeline_stats = cursor.fetchone()
            
            # Get recent pipeline runs
            cursor.execute("""
                SELECT 
                    DATE(created_date) as run_date,
                    COUNT(*) as records_processed
                FROM analytics_results 
                WHERE created_date >= DATE_SUB(NOW(), INTERVAL 7 DAY)
                GROUP BY DATE(created_date)
                ORDER BY run_date DESC
            """)
            recent_runs = cursor.fetchall()
            
            cursor.close()
            
            # Display metrics
            st.metric("Total Files", pipeline_stats['total_files'])
            st.metric("Processed Files", pipeline_stats['processed_files'])
            st.metric("Completion Rate", f"{pipeline_stats['completion_rate']:.1f}%")
            
            # Recent runs chart
            if recent_runs:
                runs_df = pd.DataFrame(recent_runs)
                fig = px.bar(
                    runs_df,
                    x='run_date',
                    y='records_processed',
                    title="Recent Pipeline Runs",
                    labels={'run_date': 'Date', 'records_processed': 'Records Processed'}
                )
                st.plotly_chart(fig, use_container_width=True)
            
        except Exception as e:
            st.error(f"Error fetching pipeline status: {str(e)}")
    
    # Pipeline Configuration
    st.subheader("Pipeline Configuration")
    
    with st.expander("ETL Settings"):
        col1, col2 = st.columns(2)
        
        with col1:
            enable_validation = st.checkbox("Enable Data Validation", value=True)
            enable_analytics = st.checkbox("Enable Analytics Generation", value=True)
            enable_ml = st.checkbox("Enable ML Features", value=False)
        
        with col2:
            max_retries = st.number_input("Max Retries", min_value=1, max_value=10, value=3)
            retry_delay = st.number_input("Retry Delay (seconds)", min_value=1, max_value=60, value=5)
        
        if st.button("Save Configuration"):
            st.success("Pipeline configuration saved!")
    
    # Pipeline Logs (placeholder)
    with st.expander("Pipeline Logs"):
        st.info("Recent pipeline logs would appear here")
        # In a real implementation, you would display actual log entries
        log_entries = [
            {"timestamp": "2024-01-15 10:30:15", "level": "INFO", "message": "ETL pipeline started"},
            {"timestamp": "2024-01-15 10:31:22", "level": "INFO", "message": "Processing batch 1/5"},
            {"timestamp": "2024-01-15 10:32:45", "level": "SUCCESS", "message": "Batch 1 completed successfully"},
        ]
        
        for log in log_entries:
            if log["level"] == "INFO":
                st.write(f"â„¹ï¸ {log['timestamp']} - {log['message']}")
            elif log["level"] == "SUCCESS":
                st.write(f"âœ… {log['timestamp']} - {log['message']}")
            elif log["level"] == "ERROR":
                st.write(f"âŒ {log['timestamp']} - {log['message']}")

def show_machine_learning(ml_models):
    st.title("ðŸ¤– Machine Learning")
    
    st.subheader("ML Model Training")
    
    # Get data for ML training
    try:
        from database.connection import db_connection
        conn = db_connection.get_connection()
        cursor = conn.cursor(dictionary=True)
        
        cursor.execute("""
            SELECT pd.data_id, pd.extracted_text, pd.word_count, pd.char_count
            FROM processed_data pd
            WHERE LENGTH(pd.extracted_text) > 100
            LIMIT 1000
        """)
        
        ml_data = cursor.fetchall()
        cursor.close()
        
        if ml_data:
            st.info(f"Found {len(ml_data)} text samples for ML training")
            
            # ML Model Selection
            st.subheader("Select ML Models to Train")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                train_clustering = st.checkbox("Text Clustering", value=True)
                if train_clustering:
                    n_clusters = st.slider("Number of Clusters", 2, 10, 5)
            
            with col2:
                train_anomaly = st.checkbox("Anomaly Detection", value=True)
                train_sentiment = st.checkbox("Sentiment Analysis", value=True)
            
            with col3:
                train_topic = st.checkbox("Topic Modeling", value=False)
                if train_topic:
                    n_topics = st.slider("Number of Topics", 2, 10, 3)
            
            if st.button("Train Selected Models", type="primary"):
                results = {}
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                texts = [item['extracted_text'] for item in ml_data]
                
                # Train clustering
                if train_clustering:
                    status_text.text("Training text clustering...")
                    try:
                        clustering_results = ml_models.train_text_clustering(texts, n_clusters=n_clusters)
                        results['clustering'] = clustering_results
                        st.success("âœ… Clustering model trained!")
                    except Exception as e:
                        st.error(f"âŒ Clustering failed: {str(e)}")
                    progress_bar.progress(25)
                
                # Train anomaly detection
                if train_anomaly:
                    status_text.text("Training anomaly detection...")
                    try:
                        features = ml_models._extract_ml_features(ml_data)
                        anomaly_results = ml_models.train_anomaly_detection(pd.DataFrame(features))
                        results['anomaly_detection'] = anomaly_results
                        st.success("âœ… Anomaly detection model trained!")
                    except Exception as e:
                        st.error(f"âŒ Anomaly detection failed: {str(e)}")
                    progress_bar.progress(50)
                
                # Train sentiment analysis
                if train_sentiment:
                    status_text.text("Training sentiment analysis...")
                    try:
                        sentiment_results = ml_models.train_sentiment_analysis(texts)
                        results['sentiment_analysis'] = sentiment_results
                        st.success("âœ… Sentiment analysis model trained!")
                    except Exception as e:
                        st.error(f"âŒ Sentiment analysis failed: {str(e)}")
                    progress_bar.progress(75)
                
                # Train topic modeling
                if train_topic:
                    status_text.text("Training topic modeling...")
                    try:
                        topic_results = ml_models.perform_topic_modeling(texts, n_topics=n_topics)
                        results['topic_modeling'] = topic_results
                        st.success("âœ… Topic modeling completed!")
                    except Exception as e:
                        st.error(f"âŒ Topic modeling failed: {str(e)}")
                    progress_bar.progress(100)
                
                status_text.text("ML training completed!")
                
                # Display results
                if results:
                    st.subheader("Model Results")
                    
                    for model_name, model_results in results.items():
                        with st.expander(f"{model_name.replace('_', ' ').title()} Results"):
                            if model_name == 'clustering':
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("Silhouette Score", f"{model_results['silhouette_score']:.3f}")
                                with col2:
                                    st.metric("Number of Clusters", model_results['n_clusters'])
                                
                                st.write("**Top Terms per Cluster:**")
                                for i, terms in enumerate(model_results['top_terms_per_cluster']):
                                    st.write(f"Cluster {i+1}: {', '.join(terms[:5])}")
                            
                            elif model_name == 'anomaly_detection':
                                anomalies = sum(1 for x in model_results['anomalies'] if x == -1)
                                st.metric("Anomalies Detected", anomalies)
                                st.metric("Contamination", f"{model_results['contamination']:.2f}")
                            
                            elif model_name == 'sentiment_analysis':
                                avg_sentiment = np.mean(model_results['predictions'])
                                avg_confidence = np.mean(model_results['confidence_scores'])
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("Average Sentiment", f"{avg_sentiment:.3f}")
                                with col2:
                                    st.metric("Average Confidence", f"{avg_confidence:.3f}")
                            
                            elif model_name == 'topic_modeling':
                                st.write("**Discovered Topics:**")
                                for i, terms in enumerate(model_results['topic_terms']):
                                    st.write(f"Topic {i+1}: {', '.join(terms[:5])}")
            
            # Advanced ML features
            st.subheader("Advanced ML Analysis")
            if st.button("Generate Comprehensive Insights"):
                with st.spinner("Generating comprehensive ML insights..."):
                    try:
                        insights = ml_models.generate_ml_insights(ml_data)
                        
                        if 'error' not in insights:
                            st.success("âœ… Comprehensive ML insights generated!")
                            
                            # Display insights in tabs
                            tab1, tab2, tab3, tab4 = st.tabs(["Clustering", "Topics", "Sentiment", "Anomalies"])
                            
                            with tab1:
                                if 'clustering' in insights:
                                    clustering = insights['clustering']
                                    st.plotly_chart(
                                        px.bar(
                                            x=list(range(len(clustering['top_terms_per_cluster']))),
                                            y=[len(terms) for terms in clustering['top_terms_per_cluster']],
                                            title="Cluster Sizes",
                                            labels={'x': 'Cluster', 'y': 'Number of Terms'}
                                        )
                                    )
                            
                            with tab2:
                                if 'topic_modeling' in insights:
                                    topic_modeling = insights['topic_modeling']
                                    st.write("Discovered Topics:")
                                    for i, terms in enumerate(topic_modeling['topic_terms']):
                                        st.write(f"**Topic {i+1}**: {', '.join(terms[:8])}")
                            
                            with tab3:
                                if 'sentiment_analysis' in insights:
                                    sentiment = insights['sentiment_analysis']
                                    sentiments = sentiment['predictions']
                                    st.plotly_chart(
                                        px.histogram(
                                            x=sentiments,
                                            title="Sentiment Distribution",
                                            labels={'x': 'Sentiment Score', 'y': 'Count'}
                                        )
                                    )
                            
                            with tab4:
                                if 'anomaly_detection' in insights:
                                    anomalies = insights['anomaly_detection']
                                    st.metric("Total Anomalies", sum(1 for x in anomalies['anomalies'] if x == -1))
                        
                        else:
                            st.error(f"âŒ Error generating insights: {insights['error']}")
                            
                    except Exception as e:
                        st.error(f"âŒ Error in comprehensive analysis: {str(e)}")
        
        else:
            st.warning("No sufficient data available for ML training. Please process more files first.")
            
    except Exception as e:
        st.error(f"Error accessing data for ML: {str(e)}")
    
    # Model Management
    st.subheader("Model Management")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Save Current Models"):
            try:
                models_dir = "data/models"
                os.makedirs(models_dir, exist_ok=True)
                model_path = os.path.join(models_dir, f"models_{Helpers.get_timestamp_string()}.joblib")
                ml_models.save_models(model_path)
                st.success(f"âœ… Models saved to {model_path}")
            except Exception as e:
                st.error(f"âŒ Error saving models: {str(e)}")
    
    with col2:
        model_files = []
        if os.path.exists("data/models"):
            model_files = [f for f in os.listdir("data/models") if f.endswith('.joblib')]
        
        if model_files:
            selected_model = st.selectbox("Select Model to Load", model_files)
            if st.button("Load Selected Model"):
                try:
                    model_path = os.path.join("data/models", selected_model)
                    ml_models.load_models(model_path)
                    st.success(f"âœ… Model {selected_model} loaded successfully")
                except Exception as e:
                    st.error(f"âŒ Error loading model: {str(e)}")
        else:
            st.info("No saved models found")

def show_data_quality(analytics_reporter):
    st.title("ðŸ” Data Quality")
    
    dashboard_data = analytics_reporter.generate_dashboard_data()
    quality_data = dashboard_data.get('quality_overview', {})
    
    if quality_data:
        # Overall Quality Score
        st.subheader("Overall Data Quality Score")
        
        total_checks = 0
        passed_checks = 0
        
        for check_type, status_count in quality_data.items():
            total_checks += sum(status_count.values())
            passed_checks += status_count.get('good', 0)
        
        overall_score = (passed_checks / total_checks * 100) if total_checks > 0 else 0
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Overall Quality Score", f"{overall_score:.1f}%")
        with col2:
            st.metric("Total Checks", total_checks)
        with col3:
            st.metric("Passed Checks", passed_checks)
        
        # Quality gauge chart
        fig = go.Figure(go.Indicator(
            mode = "gauge+number+delta",
            value = overall_score,
            domain = {'x': [0, 1], 'y': [0, 1]},
            title = {'text': "Data Quality Score"},
            gauge = {
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 50], 'color': "red"},
                    {'range': [50, 80], 'color': "yellow"},
                    {'range': [80, 100], 'color': "green"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 90
                }
            }
        ))
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Detailed Quality Metrics
        st.subheader("Detailed Quality Metrics")
        
        for check_type, status_count in quality_data.items():
            with st.expander(f"Quality Check: {check_type.title()}"):
                total = sum(status_count.values())
                good = status_count.get('good', 0)
                score = (good / total * 100) if total > 0 else 0
                
                col1, col2 = st.columns([1, 3])
                with col1:
                    st.metric("Score", f"{score:.1f}%")
                with col2:
                    st.progress(score / 100)
                
                # Status breakdown
                status_df = pd.DataFrame([
                    {'Status': status, 'Count': count}
                    for status, count in status_count.items()
                ])
                
                if not status_df.empty:
                    fig = px.pie(
                        status_df,
                        values='Count',
                        names='Status',
                        title=f"{check_type.title()} Status Distribution"
                    )
                    st.plotly_chart(fig, use_container_width=True)
    
    else:
        st.info("No quality data available. Run the ETL pipeline first to generate quality metrics.")
    
    # Data Quality Rules
    st.subheader("Data Quality Rules")
    
    with st.expander("Configure Quality Rules"):
        st.write("Define data quality rules and thresholds:")
        
        col1, col2 = st.columns(2)
        
        with col1:
            min_word_count = st.number_input("Minimum Word Count", min_value=1, value=10)
            max_file_size = st.number_input("Maximum File Size (MB)", min_value=1, value=100)
            require_text_content = st.checkbox("Require Text Content", value=True)
        
        with col2:
            check_completeness = st.checkbox("Check Completeness", value=True)
            check_validity = st.checkbox("Check Validity", value=True)
            check_consistency = st.checkbox("Check Consistency", value=True)
        
        if st.button("Save Quality Rules"):
            st.success("Quality rules saved successfully!")
    
    # Quality Issues
    st.subheader("Recent Quality Issues")
    
    try:
        from database.connection import db_connection
        conn = db_connection.get_connection()
        cursor = conn.cursor(dictionary=True)
        
        cursor.execute("""
            SELECT dqm.check_type, dqm.status, dqm.check_value, dqm.check_date, f.filename
            FROM data_quality_metrics dqm
            JOIN files f ON dqm.file_id = f.file_id
            WHERE dqm.status != 'good'
            ORDER BY dqm.check_date DESC
            LIMIT 20
        """)
        
        quality_issues = cursor.fetchall()
        cursor.close()
        
        if quality_issues:
            issues_df = pd.DataFrame(quality_issues)
            st.dataframe(issues_df, use_container_width=True)
        else:
            st.success("No quality issues found! All checks are passing.")
            
    except Exception as e:
        st.error(f"Error fetching quality issues: {str(e)}")

def show_analytics(analytics_reporter):
    st.title("ðŸ“ˆ Analytics & Insights")
    
    dashboard_data = analytics_reporter.generate_dashboard_data()
    insights = dashboard_data.get('analytics_insights', {})
    
    if insights and insights.get('averages'):
        averages = insights['averages']
        
        st.subheader("Key Analytics Metrics")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            avg_words = averages.get('avg_word_count', 0)
            st.metric("Average Word Count", f"{avg_words:.0f}")
        
        with col2:
            avg_sentiment = averages.get('avg_sentiment', 0)
            st.metric("Average Sentiment", f"{avg_sentiment:.3f}")
        
        with col3:
            avg_confidence = averages.get('avg_confidence', 0)
            st.metric("Average Confidence", f"{avg_confidence:.3f}")
        
        with col4:
            total_analytics = len(insights.get('recent_activity', []))
            st.metric("Total Analyses", total_analytics)
    
    # Advanced analytics charts
    st.subheader("Advanced Analytics")
    
    try:
        from database.connection import db_connection
        conn = db_connection.get_connection()
        cursor = conn.cursor(dictionary=True)
        
        # File type analytics
        cursor.execute("""
            SELECT file_type, COUNT(*) as count, AVG(file_size) as avg_size
            FROM files 
            GROUP BY file_type 
            ORDER BY count DESC
        """)
        file_type_stats = cursor.fetchall()
        
        if file_type_stats:
            file_df = pd.DataFrame(file_type_stats)
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.bar(
                    file_df,
                    x='file_type',
                    y='count',
                    title="File Type Distribution",
                    color='file_type'
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                fig = px.pie(
                    file_df,
                    values='count',
                    names='file_type',
                    title="File Type Proportion"
                )
                st.plotly_chart(fig, use_container_width=True)
        
        # Processing timeline
        cursor.execute("""
            SELECT DATE(upload_date) as date, COUNT(*) as uploads
            FROM files 
            WHERE upload_date >= DATE_SUB(NOW(), INTERVAL 30 DAY)
            GROUP BY DATE(upload_date)
            ORDER BY date
        """)
        upload_timeline = cursor.fetchall()
        
        if upload_timeline:
            timeline_df = pd.DataFrame(upload_timeline)
            fig = px.line(
                timeline_df,
                x='date',
                y='uploads',
                title="File Upload Timeline (Last 30 Days)",
                markers=True
            )
            st.plotly_chart(fig, use_container_width=True)
        
        cursor.close()
        
    except Exception as e:
        st.error(f"Error fetching analytics data: {str(e)}")
    
    # Custom Analytics Queries
    st.subheader("Custom Analytics")
    
    with st.expander("Run Custom Analysis"):
        custom_query = st.text_area(
            "Enter SQL Query",
            value="SELECT file_type, COUNT(*) as count FROM files GROUP BY file_type",
            height=100
        )
        
        if st.button("Run Query"):
            try:
                from database.connection import db_connection
                conn = db_connection.get_connection()
                df = pd.read_sql(custom_query, conn)
                
                st.success("Query executed successfully!")
                st.dataframe(df, use_container_width=True)
                
                # Basic visualization
                if len(df) > 0 and len(df.columns) >= 2:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        try:
                            fig = px.bar(df, x=df.columns[0], y=df.columns[1])
                            st.plotly_chart(fig, use_container_width=True)
                        except:
                            st.info("Could not generate bar chart")
                    
                    with col2:
                        try:
                            fig = px.pie(df, values=df.columns[1], names=df.columns[0])
                            st.plotly_chart(fig, use_container_width=True)
                        except:
                            st.info("Could not generate pie chart")
                
            except Exception as e:
                st.error(f"Query execution error: {str(e)}")
    
    # Export Analytics
    st.subheader("Export Analytics")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("Export to CSV"):
            try:
                from database.connection import db_connection
                conn = db_connection.get_connection()
                
                # Export files data
                files_df = pd.read_sql("SELECT * FROM files", conn)
                csv = files_df.to_csv(index=False)
                
                st.download_button(
                    label="Download Files CSV",
                    data=csv,
                    file_name=f"files_export_{Helpers.get_timestamp_string()}.csv",
                    mime="text/csv"
                )
                
            except Exception as e:
                st.error(f"Export error: {str(e)}")
    
    with col2:
        if st.button("Generate Report"):
            with st.spinner("Generating comprehensive report..."):
                try:
                    # Generate a comprehensive report
                    st.success("Report generated successfully!")
                    
                    report_data = {
                        "generated_at": datetime.now().isoformat(),
                        "total_files": dashboard_data['processing_stats']['total_files'],
                        "processing_rate": dashboard_data['processing_stats']['processing_rate'],
                        "quality_score": "85%",  # This would be calculated
                        "insights": "Sample insights report"
                    }
                    
                    st.json(report_data)
                    
                except Exception as e:
                    st.error(f"Report generation error: {str(e)}")
    
    with col3:
        if st.button("Clear Analytics Cache"):
            st.cache_resource.clear()
            st.success("Analytics cache cleared!")
            st.rerun()

def show_database_explorer():
    st.title("ðŸ—ƒï¸ Database Explorer")
    
    try:
        from database.connection import db_connection
        conn = db_connection.get_connection()
        cursor = conn.cursor(dictionary=True)
        
        # Get table information
        cursor.execute("SHOW TABLES")
        tables = cursor.fetchall()
        table_names = [list(table.values())[0] for table in tables]
        
        if not table_names:
            st.error("No tables found in the database")
            return
        
        # Table selection and exploration
        selected_table = st.selectbox("Select Table", table_names)
        
        if selected_table:
            # Table info
            st.subheader(f"Table: {selected_table}")
            
            # Get column information
            cursor.execute(f"DESCRIBE {selected_table}")
            columns = cursor.fetchall()
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Columns:**")
                columns_df = pd.DataFrame(columns)
                st.dataframe(columns_df, use_container_width=True)
            
            with col2:
                # Row count
                cursor.execute(f"SELECT COUNT(*) as row_count FROM {selected_table}")
                row_count = cursor.fetchone()['row_count']
                st.metric("Total Rows", row_count)
                
                # Sample data
                st.write("**Sample Data:**")
                cursor.execute(f"SELECT * FROM {selected_table} LIMIT 5")
                sample_data = cursor.fetchall()
                
                if sample_data:
                    sample_df = pd.DataFrame(sample_data)
                    st.dataframe(sample_df, use_container_width=True)
                else:
                    st.info("No data in selected table")
            
            # Full table data with pagination
            st.subheader("Full Table Data")
            
            # Pagination
            page_size = st.selectbox("Rows per page", [10, 25, 50, 100], index=0)
            
            if 'current_page' not in st.session_state:
                st.session_state.current_page = 0
            
            total_pages = (row_count + page_size - 1) // page_size
            
            col1, col2, col3 = st.columns([1, 2, 1])
            with col1:
                if st.button("Previous") and st.session_state.current_page > 0:
                    st.session_state.current_page -= 1
            with col2:
                st.write(f"Page {st.session_state.current_page + 1} of {total_pages}")
            with col3:
                if st.button("Next") and st.session_state.current_page < total_pages - 1:
                    st.session_state.current_page += 1
            
            # Fetch data for current page
            offset = st.session_state.current_page * page_size
            cursor.execute(f"SELECT * FROM {selected_table} LIMIT {page_size} OFFSET {offset}")
            page_data = cursor.fetchall()
            
            if page_data:
                df = pd.DataFrame(page_data)
                st.dataframe(df, use_container_width=True)
            else:
                st.info("No data to display")
            
            # Export options
            st.subheader("Export Data")
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("Export Current Page to CSV"):
                    if page_data:
                        csv = df.to_csv(index=False)
                        st.download_button(
                            label="Download CSV",
                            data=csv,
                            file_name=f"{selected_table}_page_{st.session_state.current_page + 1}.csv",
                            mime="text/csv"
                        )
            
            with col2:
                if st.button("Export Entire Table to CSV"):
                    cursor.execute(f"SELECT * FROM {selected_table}")
                    all_data = cursor.fetchall()
                    full_df = pd.DataFrame(all_data)
                    csv = full_df.to_csv(index=False)
                    
                    st.download_button(
                        label="Download Full CSV",
                        data=csv,
                        file_name=f"{selected_table}_full_{Helpers.get_timestamp_string()}.csv",
                        mime="text/csv"
                    )
        
        cursor.close()
        
    except Exception as e:
        st.error(f"Error accessing database: {str(e)}")

if __name__ == "__main__":
    main()